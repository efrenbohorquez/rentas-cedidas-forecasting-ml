Total celdas: 29

=== Celda 0 [markdown] ===
# AnÃ¡lisis Exploratorio de Datos y DepuraciÃ³n
# Recaudo de Rentas Cedidas - Series de Tiempo 2026

---

## ğŸ“Š Objetivo

Este cuaderno implementa el **diagnÃ³stico completo y depuraciÃ³n estructural** de los datos de recaudo para preparar el modelado predictivo con tÃ©cnicas de series de tiempo de Ãºltima generaciÃ³n (2026).

### Alcance del AnÃ¡lisis:
1. **ExploraciÃ³n inicial**: Estructura, dimensiones, tipos de datos
2. **DiagnÃ³stico de calidad**: Valores faltantes, duplicados, inconsistencias
3. **IdentificaciÃ³n de anomalÃ­as**: Outliers extremos, valores negativos, transacciones en escala cero
4. **DepuraciÃ³n estructural**: Limpieza, normalizaciÃ³n, validaciÃ³n de integridad
5. **AnÃ¡lisis temporal**: Rangos, gaps, estacionalidad preliminar

---

**Autor**: Sistema de AnÃ¡lisis Predictivo  
**Fecha**: Febrero 2026  
**VersiÃ³n**: 1.0

=== Celda 1 [markdown] ===
## 1. ConfiguraciÃ³n del Entorno

=== Celda 2 [code] ===
pip install missingno

=== Celda 3 [code] ===
# Importar bibliotecas necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
from datetime import datetime
import missingno as msno
from scipy import stats
from sklearn.ensemble import IsolationForest

# ConfiguraciÃ³n de visualizaciÃ³n
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
%matplotlib inline

# ConfiguraciÃ³n de pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.2f}'.format)

print("âœ… Bibliotecas cargadas exitosamente")
print(f"ğŸ“… Fecha de ejecuciÃ³n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

=== Celda 4 [markdown] ===
## 2. Carga de Datos

=== Celda 5 [code] ===
# Cargar datos desde Excel
print("ğŸ“ Cargando datos de BaseRentasVF_limpieza21feb.xlsx...")
df_raw = pd.read_excel('D:\\RENTAS\\data\\BaseRentasVF_limpieza21feb.xlsx')

print(f"\nâœ… Datos cargados exitosamente")
print(f"ğŸ“Š Dimensiones: {df_raw.shape[0]:,} filas Ã— {df_raw.shape[1]} columnas")
print(f"ğŸ’¾ Memoria utilizada: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

=== Celda 6 [markdown] ===
## 3. ExploraciÃ³n Inicial de Estructura

=== Celda 7 [code] ===
# InformaciÃ³n general del dataset
print("=" * 80)
print("INFORMACIÃ“N GENERAL DEL DATASET")
print("=" * 80)
df_raw.info()

print("\n" + "=" * 80)
print("PRIMERAS 10 FILAS")
print("=" * 80)
display(df_raw.head(10))

print("\n" + "=" * 80)
print("COLUMNAS DISPONIBLES")
print("=" * 80)
for i, col in enumerate(df_raw.columns, 1):
    print(f"{i:2d}. {col}")

=== Celda 8 [markdown] ===
# Identificar columnas con datos temporales y de recaudo
columnas_temporales = [col for col in df_raw.columns if any(x in col.lower() for x in ['vigencia', 'fecha', 'periodo', 'aÃ±o', 'mes'])]
columnas_recaudo = [col for col in df_raw.columns if any(x in col.lower() for x in ['valor', 'monto', 'ingreso'])]
# Nota: Se removiÃ³ 'recaudo' del filtro de recaudo para evitar que 'FechaRecaudo' aparezca como columna de recaudo

print("ğŸ—“ï¸  COLUMNAS TEMPORALES DETECTADAS:")
for col in columnas_temporales:
    print(f"  - {col}")
    if col in df_raw.columns:
        print(f"    Valores Ãºnicos: {df_raw[col].nunique()}")
        print(f"    Rango: {df_raw[col].min()} - {df_raw[col].max()}")
        print(f"    Tipo: {df_raw[col].dtype}")
        # DiagnÃ³stico especial para MesNombreCalendario
        if 'mes' in col.lower() and df_raw[col].dtype == 'object':
            valores = df_raw[col].unique()
            print(f"    âš ï¸  Valores encontrados ({len(valores)}): {sorted(valores)}")
            # Normalizar a tÃ­tulo (ej: "Enero") para ver cuÃ¡ntos meses reales hay
            meses_normalizados = df_raw[col].str.strip().str.capitalize().nunique()
            print(f"    âœ… Meses reales (tras normalizar capitalizaciÃ³n): {meses_normalizados}")
        print()

print("\nğŸ’° COLUMNAS DE RECAUDO DETECTADAS:")
for col in columnas_recaudo:
    print(f"  - {col}")
    if col in df_raw.columns:
        print(f"    Valores no nulos: {df_raw[col].notna().sum():,} ({df_raw[col].notna().sum()/len(df_raw)*100:.1f}%)")
        print(f"    Tipo: {df_raw[col].dtype}")
        # Alerta si el tipo no es numÃ©rico
        if df_raw[col].dtype == 'object':
            print(f"    âš ï¸  ALERTA: Esta columna es tipo texto. Requiere conversiÃ³n a numÃ©rico.")
            print(f"    Muestra de valores: {df_raw[col].head(5).tolist()}")
        print()

=== Celda 9 [code] ===
# ===== LIMPIEZA PREVENTIVA =====
# 1. Normalizar MesNombreCalendario (corregir capitalizaciÃ³n inconsistente)
if '  MesNombreCalendario' in df_raw.columns:
    col_mes = '  MesNombreCalendario'
elif 'MesNombreCalendario' in df_raw.columns:
    col_mes = 'MesNombreCalendario'
else:
    col_mes = [c for c in df_raw.columns if 'mes' in c.lower() and 'nombre' in c.lower()]
    col_mes = col_mes[0] if col_mes else None

if col_mes:
    antes = df_raw[col_mes].nunique()
    df_raw[col_mes] = df_raw[col_mes].str.strip().str.capitalize()
    despues = df_raw[col_mes].nunique()
    print(f"âœ… '{col_mes}' normalizado: {antes} valores Ãºnicos â†’ {despues} valores Ãºnicos")
    print(f"   Meses resultantes: {sorted(df_raw[col_mes].unique())}")

# 2. Convertir ValorRecaudo de texto a numÃ©rico
if '  ValorRecaudo' in df_raw.columns:
    col_valor = '  ValorRecaudo'
elif 'ValorRecaudo' in df_raw.columns:
    col_valor = 'ValorRecaudo'
else:
    col_valor = [c for c in df_raw.columns if 'valor' in c.lower() and 'recaudo' in c.lower()]
    col_valor = col_valor[0] if col_valor else None

if col_valor and df_raw[col_valor].dtype == 'object':
    print(f"\nğŸ”§ Convirtiendo '{col_valor}' de texto a numÃ©rico...")
    # Limpiar: quitar $, espacios, puntos de miles, reemplazar coma decimal
    df_raw[col_valor] = (
        df_raw[col_valor]
        .astype(str)
        .str.replace('$', '', regex=False)
        .str.replace(' ', '', regex=False)
        .str.strip()
    )
    df_raw[col_valor] = pd.to_numeric(df_raw[col_valor], errors='coerce')
    nulos_creados = df_raw[col_valor].isna().sum()
    print(f"âœ… '{col_valor}' convertido a {df_raw[col_valor].dtype}")
    if nulos_creados > 0:
        print(f"âš ï¸  {nulos_creados:,} valores no pudieron convertirse (quedaron como NaN)")
    else:
        print(f"âœ… Todos los valores convertidos exitosamente")
    print(f"   Rango: {df_raw[col_valor].min():,.2f} - {df_raw[col_valor].max():,.2f}")

=== Celda 10 [markdown] ===
## 4. IdentificaciÃ³n de Columnas Clave para Series de Tiempo

Para el anÃ¡lisis de series de tiempo, necesitamos identificar:
- **Variable temporal**: Vigencia, Fecha, Periodo, etc.
- **Variable objetivo (target)**: Valor de recaudo
- **Variables auxiliares**: Tipo de renta, entidad, categorÃ­a, etc.

=== Celda 11 [markdown] ===
## 5. DiagnÃ³stico de Calidad de Datos

=== Celda 12 [code] ===
# AnÃ¡lisis de valores faltantes
print("=" * 80)
print("ANÃLISIS DE VALORES FALTANTES")
print("=" * 80)

missing_data = pd.DataFrame({
    'Columna': df_raw.columns,
    'Valores_Faltantes': df_raw.isnull().sum(),
    'Porcentaje': (df_raw.isnull().sum() / len(df_raw) * 100).round(2)
}).sort_values('Porcentaje', ascending=False)

missing_data = missing_data[missing_data['Valores_Faltantes'] > 0]

if len(missing_data) > 0:
    display(missing_data)
    
    # VisualizaciÃ³n con missingno
    fig, ax = plt.subplots(figsize=(14, 6))
    msno.matrix(df_raw, ax=ax, sparkline=False)
    plt.title('Matriz de Valores Faltantes', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
else:
    print("âœ… No se detectaron valores faltantes")

=== Celda 13 [code] ===
# AnÃ¡lisis de duplicados
print("=" * 80)
print("ANÃLISIS DE REGISTROS DUPLICADOS")
print("=" * 80)

duplicados_totales = df_raw.duplicated().sum()
print(f"Duplicados completos: {duplicados_totales:,} ({duplicados_totales/len(df_raw)*100:.2f}%)")

if duplicados_totales > 0:
    print("\nâš ï¸  Se detectaron registros duplicados que requieren revisiÃ³n")
else:
    print("âœ… No se detectaron duplicados completos")

=== Celda 14 [markdown] ===
## 6. PreparaciÃ³n de Datos para AnÃ¡lisis Temporal



=== Celda 15 [code] ===


=== Celda 16 [code] ===
# â”€â”€ NormalizaciÃ³n de nombres de columnas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RENAME_COLS = {
    'FechaRecaudo'                : 'FechaRecaudo',
    'NitBeneficiarioAportante'    : 'NitBeneficiarioAportante',
    'NombreBeneficiarioAportante' : 'NombreBeneficiarioAportante',
    'ValorRecaudo'                : 'ValorRecaudo',
    'Nombre de Rubro'             : 'NombreRubro',          # espacio â†’ sin espacio
    'NombreGrupoFuente'           : 'NombreGrupoFuente',
    'CÃ³digoSubGrupoFuente'        : 'CodigoSubGrupoFuente', # acento eliminado
    'NombreSubGrupoFuente'        : 'NombreSubGrupoFuente',
    'CÃ³digoConcepto'              : 'CodigoConcepto',       # acento eliminado
    'NombreConcepto'              : 'NombreConcepto',
    'CÃ³digoTipoRegistro'          : 'CodigoTipoRegistro',   # acento eliminado
    'TipoRegistro'                : 'TipoRegistro',
    'Nombre_SubGrupo_Aportante'   : 'Nombre_SubGrupo_Aportante',
}

df_raw = df_raw.rename(columns=RENAME_COLS)
print("âœ… Columnas normalizadas:", df_raw.columns.tolist())

=== Celda 17 [markdown] ===
## 7. AnÃ¡lisis EstadÃ­stico de la Variable Objetivo (Recaudo)

=== Celda 18 [code] ===
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. ANÃLISIS ESTADÃSTICO DE LA VARIABLE OBJETIVO (RECAUDO)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

target = 'ValorRecaudo'
s = df_raw[target].dropna()

# â”€â”€ 7.1 EstadÃ­sticas descriptivas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
desc = s.describe(percentiles=[.05, .25, .5, .75, .95])
iqr  = desc['75%'] - desc['25%']
cv   = (s.std() / s.mean()) * 100
skew = s.skew()
kurt = s.kurtosis()

print("=" * 55)
print("  ESTADÃSTICAS DESCRIPTIVAS â€” ValorRecaudo")
print("=" * 55)
print(f"  N (total)        : {len(s):>15,.0f}")
print(f"  Media            : {s.mean():>15,.2f}")
print(f"  Mediana          : {s.median():>15,.2f}")
print(f"  Desv. estÃ¡ndar   : {s.std():>15,.2f}")
print(f"  CV (%)           : {cv:>15.1f}")
print(f"  MÃ­nimo           : {s.min():>15,.2f}")
print(f"  P5               : {desc['5%']:>15,.2f}")
print(f"  P25              : {desc['25%']:>15,.2f}")
print(f"  P75              : {desc['75%']:>15,.2f}")
print(f"  P95              : {desc['95%']:>15,.2f}")
print(f"  MÃ¡ximo           : {s.max():>15,.2f}")
print(f"  IQR              : {iqr:>15,.2f}")
print(f"  AsimetrÃ­a        : {skew:>15.4f}")
print(f"  Curtosis         : {kurt:>15.4f}")
print("=" * 55)

# â”€â”€ 7.2 Prueba de normalidad (Shapiro si n<5000, else KS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if len(s) <= 5000:
    stat, p = stats.shapiro(s)
    test_name = "Shapiro-Wilk"
else:
    stat, p = stats.kstest(s, 'norm', args=(s.mean(), s.std()))
    test_name = "Kolmogorov-Smirnov"

print(f"\n  Prueba {test_name}: stat={stat:.4f}, p={p:.4e}")
print(f"  â†’ {'NO normal (p<0.05)' if p < 0.05 else 'Normal (pâ‰¥0.05)'}")

# â”€â”€ 7.3 Visualizaciones â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle("AnÃ¡lisis EstadÃ­stico â€” ValorRecaudo", fontsize=14, fontweight='bold')

# Histograma + KDE
ax = axes[0, 0]
ax.hist(s, bins=60, color='steelblue', edgecolor='white', alpha=0.8, density=True)
s.plot(kind='kde', ax=ax, color='crimson', linewidth=2)
ax.set_title("DistribuciÃ³n (Histograma + KDE)")
ax.set_xlabel("ValorRecaudo"); ax.set_ylabel("Densidad")

# Boxplot
ax = axes[0, 1]
ax.boxplot(s, vert=False, patch_artist=True,
           boxprops=dict(facecolor='steelblue', alpha=0.6),
           medianprops=dict(color='crimson', linewidth=2))
ax.set_title("Boxplot â€” DispersiÃ³n y Outliers")
ax.set_xlabel("ValorRecaudo")

# Log-transformada
ax = axes[1, 0]
s_log = np.log1p(s[s > 0])
ax.hist(s_log, bins=60, color='seagreen', edgecolor='white', alpha=0.8, density=True)
s_log.plot(kind='kde', ax=ax, color='darkorange', linewidth=2)
ax.set_title("DistribuciÃ³n log1p(ValorRecaudo)")
ax.set_xlabel("log1p(ValorRecaudo)"); ax.set_ylabel("Densidad")

# Q-Q plot
ax = axes[1, 1]
(osm, osr), (slope, intercept, r) = stats.probplot(s, dist="norm")
ax.scatter(osm, osr, s=6, color='steelblue', alpha=0.5)
ax.plot(osm, slope * np.array(osm) + intercept, color='crimson', linewidth=1.5)
ax.set_title(f"Q-Q Plot Normal (RÂ²={r**2:.4f})")
ax.set_xlabel("Cuantiles teÃ³ricos"); ax.set_ylabel("Cuantiles observados")

plt.tight_layout()
plt.show()

# â”€â”€ 7.4 DetecciÃ³n de outliers (IQR) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
lower = desc['25%'] - 1.5 * iqr
upper = desc['75%'] + 1.5 * iqr
out_mask = (s < lower) | (s > upper)
print(f"\n  Outliers IQR: {out_mask.sum():,} ({out_mask.mean()*100:.1f}% del total)")
print(f"  LÃ­mite inferior: {lower:,.2f}")
print(f"  LÃ­mite superior: {upper:,.2f}")

=== Celda 19 [markdown] ===
## 8. VisualizaciÃ³n de DistribuciÃ³n de Recaudo

=== Celda 20 [code] ===
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. ANÃLISIS ESTADÃSTICO DE LA VARIABLE OBJETIVO (RECAUDO)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

target = 'ValorRecaudo'
s = df_raw[target].dropna()

# â”€â”€ 7.1 EstadÃ­sticas descriptivas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
desc = s.describe(percentiles=[.05, .25, .5, .75, .95])
iqr  = desc['75%'] - desc['25%']
cv   = (s.std() / s.mean()) * 100

print("=" * 55)
print("  ESTADÃSTICAS DESCRIPTIVAS â€” ValorRecaudo")
print("=" * 55)
print(f"  N (total)        : {len(s):>15,.0f}")
print(f"  Media            : {s.mean():>15,.2f}")
print(f"  Mediana          : {s.median():>15,.2f}")
print(f"  Desv. estÃ¡ndar   : {s.std():>15,.2f}")
print(f"  CV (%)           : {cv:>15.1f}")
print(f"  MÃ­nimo           : {s.min():>15,.2f}")
print(f"  P5               : {desc['5%']:>15,.2f}")
print(f"  P25              : {desc['25%']:>15,.2f}")
print(f"  P75              : {desc['75%']:>15,.2f}")
print(f"  P95              : {desc['95%']:>15,.2f}")
print(f"  MÃ¡ximo           : {s.max():>15,.2f}")
print(f"  IQR              : {iqr:>15,.2f}")
print(f"  AsimetrÃ­a        : {s.skew():>15.4f}")
print(f"  Curtosis         : {s.kurtosis():>15.4f}")
print("=" * 55)

# â”€â”€ 7.2 Prueba de normalidad â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if len(s) <= 5000:
    stat, p = stats.shapiro(s)
    test_name = "Shapiro-Wilk"
else:
    stat, p = stats.kstest(s, 'norm', args=(s.mean(), s.std()))
    test_name = "Kolmogorov-Smirnov"

print(f"\n  Prueba {test_name}: stat={stat:.4f}, p={p:.4e}")
print(f"  â†’ {'NO normal (p<0.05)' if p < 0.05 else 'Normal (pâ‰¥0.05)'}")

# â”€â”€ 7.3 Visualizaciones â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle("AnÃ¡lisis EstadÃ­stico â€” ValorRecaudo", fontsize=14, fontweight='bold')

# Histograma + KDE
ax = axes[0, 0]
ax.hist(s, bins=60, color='steelblue', edgecolor='white', alpha=0.8, density=True)
s.plot(kind='kde', ax=ax, color='crimson', linewidth=2)
ax.set_title("DistribuciÃ³n (Histograma + KDE)")
ax.set_xlabel("ValorRecaudo"); ax.set_ylabel("Densidad")

# Boxplot  â† 'orientation' reemplaza 'vert' en matplotlib â‰¥ 3.9
ax = axes[0, 1]
bp_kwargs = dict(patch_artist=True,
                 boxprops=dict(facecolor='steelblue', alpha=0.6),
                 medianprops=dict(color='crimson', linewidth=2))
try:
    ax.boxplot(s, orientation='horizontal', **bp_kwargs)
except TypeError:           # matplotlib < 3.9
    ax.boxplot(s, vert=False, **bp_kwargs)
ax.set_title("Boxplot â€” DispersiÃ³n y Outliers")
ax.set_xlabel("ValorRecaudo")

# Log-transformada
ax = axes[1, 0]
s_log = np.log1p(s[s > 0])
ax.hist(s_log, bins=60, color='seagreen', edgecolor='white', alpha=0.8, density=True)
s_log.plot(kind='kde', ax=ax, color='darkorange', linewidth=2)
ax.set_title("DistribuciÃ³n log1p(ValorRecaudo)")
ax.set_xlabel("log1p(ValorRecaudo)"); ax.set_ylabel("Densidad")

# Q-Q plot
ax = axes[1, 1]
(osm, osr), (slope, intercept, r) = stats.probplot(s, dist='norm')
ax.scatter(osm, osr, s=6, color='steelblue', alpha=0.5)
ax.plot(osm, slope * np.array(osm) + intercept, color='crimson', linewidth=1.5)
ax.set_title(f"Q-Q Plot Normal (RÂ²={r**2:.4f})")
ax.set_xlabel("Cuantiles teÃ³ricos"); ax.set_ylabel("Cuantiles observados")

plt.tight_layout()
plt.show()

# â”€â”€ 7.4 DetecciÃ³n de outliers (IQR) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
lower = desc['25%'] - 1.5 * iqr
upper = desc['75%'] + 1.5 * iqr
out_mask = (s < lower) | (s > upper)
print(f"\n  Outliers IQR : {out_mask.sum():,} ({out_mask.mean()*100:.1f}% del total)")
print(f"  LÃ­mite inferior: {lower:,.2f}")
print(f"  LÃ­mite superior: {upper:,.2f}")

=== Celda 21 [markdown] ===
## 9. DetecciÃ³n de Outliers con Isolation Forest

=== Celda 22 [code] ===
if columna_recaudo and columna_recaudo in df_raw.columns:
    recaudo_valido = df_raw[columna_recaudo].replace([np.inf, -np.inf], np.nan).dropna()
    
    if len(recaudo_valido) > 100:
        print("ğŸ” Detectando outliers con Isolation Forest...")
        
        # Preparar datos
        X = recaudo_valido.values.reshape(-1, 1)
        
        # Entrenar modelo
        iso_forest = IsolationForest(contamination=0.05, random_state=42)
        outlier_labels = iso_forest.fit_predict(X)
        
        # Contar outliers
        n_outliers = (outlier_labels == -1).sum()
        print(f"\nâš ï¸  Outliers detectados: {n_outliers:,} ({n_outliers/len(recaudo_valido)*100:.2f}%)")
        
        # Valores extremos
        outliers = recaudo_valido[outlier_labels == -1]
        print(f"\nTop 10 outliers mÃ¡s extremos:")
        print(outliers.nlargest(10).apply(lambda x: f"${x:,.2f}"))

=== Celda 23 [markdown] ===
## 10. AnÃ¡lisis Temporal Preliminar

Explorar cÃ³mo se distribuye el recaudo a lo largo del tiempo

=== Celda 24 [code] ===
# Este cÃ³digo necesita ajustarse segÃºn la estructura real de los datos
# Por ahora, dejamos placeholder para completar una vez se explore el dataset

print("â³ Este anÃ¡lisis se completarÃ¡ una vez se identifique la estructura temporal de los datos")
print("\nPrÃ³ximos pasos:")
print("1. Identificar columnas de fecha/periodo")
print("2. Agregar recaudo por periodo (mensual/trimestral/semestral)")
print("3. Analizar estacionalidad y tendencias")
print("4. Identificar gaps temporales")

=== Celda 25 [markdown] ===
## 11. Resumen Ejecutivo de DiagnÃ³stico

### Hallazgos CrÃ­ticos Identificados

=== Celda 26 [code] ===
print("=" * 80)
print("RESUMEN EJECUTIVO - DIAGNÃ“STICO DE DATOS")
print("=" * 80)

print("\nğŸ“Š DIMENSIONES:")
print(f"  Registros totales: {len(df_raw):,}")
print(f"  Columnas: {df_raw.shape[1]}")

if columna_recaudo and columna_recaudo in df_raw.columns:
    recaudo_valido = df_raw[columna_recaudo].replace([np.inf, -np.inf], np.nan).dropna()
    
    print("\nâš ï¸  PROBLEMAS DETECTADOS:")
    
    # Valores negativos
    negativos = (recaudo_valido < 0).sum()
    if negativos > 0:
        print(f"  âŒ Valores negativos: {negativos:,} registros")
    
    # Valores extremos
    extremos = (recaudo_valido > 8e8).sum()
    if extremos > 0:
        print(f"  ğŸš¨ Outliers extremos (>$800M): {extremos:,} registros")
    
    # Valores cercanos a cero
    cerca_cero = ((recaudo_valido >= 0) & (recaudo_valido < 1000)).sum()
    if cerca_cero > 10000:
        print(f"  ğŸ“‰ Transacciones en escala cero (<$1,000): {cerca_cero:,} registros")
    
    print("\nâœ… ACCIONES REQUERIDAS:")
    print("  1. Auditar y corregir valores negativos")
    print("  2. Aplicar Winsorization/Capping a outliers extremos")
    print("  3. Filtrar o normalizar transacciones de bajo valor")
    print("  4. Validar integridad temporal")
    print("  5. Preparar agregaciones por horizonte temporal")

print("\n" + "=" * 80)

=== Celda 27 [markdown] ===
## 12. Exportar Datos para Siguiente Fase

Guardar datos crudos para continuar con Feature Engineering

=== Celda 28 [code] ===
# Guardar copia de datos originales
output_path = '../data/raw/datos_originales.parquet'
df_raw.to_parquet(output_path, index=False)
print(f"âœ… Datos originales guardados en: {output_path}")

print("\nğŸ“‹ SIGUIENTE PASO:")
print("   Abrir cuaderno: 02_Feature_Engineering.ipynb")

